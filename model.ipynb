{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whitechocobread/Ai-project/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sj_7aW-z1qLS",
        "outputId": "d959f055-209d-45e3-9a33-67e5e34187a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyautogui in c:\\users\\user\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (0.9.54)"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Requirement already satisfied: pymsgbox in c:\\users\\user\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from pyautogui) (1.0.9)\n",
            "Requirement already satisfied: pytweening>=1.0.4 in c:\\users\\user\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from pyautogui) (1.0.7)\n",
            "Requirement already satisfied: pyscreeze>=0.1.21 in c:\\users\\user\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from pyautogui) (0.1.29)\n",
            "Requirement already satisfied: pygetwindow>=0.0.5 in c:\\users\\user\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from pyautogui) (0.0.9)\n",
            "Requirement already satisfied: mouseinfo in c:\\users\\user\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from pyautogui) (0.1.3)\n",
            "Requirement already satisfied: pyrect in c:\\users\\user\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from pygetwindow>=0.0.5->pyautogui) (0.2.0)\n",
            "Requirement already satisfied: pyscreenshot in c:\\users\\user\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from pyscreeze>=0.1.21->pyautogui) (3.1)\n",
            "Requirement already satisfied: Pillow>=9.3.0 in c:\\users\\user\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from pyscreeze>=0.1.21->pyautogui) (10.1.0)\n",
            "Requirement already satisfied: pyperclip in c:\\users\\user\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from mouseinfo->pyautogui) (1.8.2)\n",
            "Requirement already satisfied: EasyProcess in c:\\users\\user\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from pyscreenshot->pyscreeze>=0.1.21->pyautogui) (1.1)\n",
            "Requirement already satisfied: entrypoint2 in c:\\users\\user\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from pyscreenshot->pyscreeze>=0.1.21->pyautogui) (1.1)\n",
            "Requirement already satisfied: mss in c:\\users\\user\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from pyscreenshot->pyscreeze>=0.1.21->pyautogui) (9.0.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install pyautogui"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvrozsi01qLU",
        "outputId": "6ecda52b-dcf4-4cdb-de97-c21becdb6d9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0                                  아 더빙.. 진짜 짜증나네요 목소리\n",
              "1                    흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
              "2                                    너무재밓었다그래서보는것을추천한다\n",
              "3                        교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n",
              "4    사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...\n",
              "Name: document, dtype: object"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "DATA_PATH = './DATA/' # 데이터 경로 설정\n",
        "train_data = pd.read_csv(DATA_PATH+'ratings_train.txt', header = 0, delimiter='\\t', quoting=3)\n",
        "\n",
        "train_data['document'][:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPfcfWTi1qLV"
      },
      "outputs": [],
      "source": [
        "#전처리 함수 만들기\n",
        "def preprocessing(review, okt, remove_stopwords = False, stop_words =[]):\n",
        "  #함수인자설명\n",
        "  # review: 전처리할 텍스트\n",
        "  # okt: okt객체를 반복적으로 생성하지 않고 미리 생성 후 인자로 받음\n",
        "  # remove_stopword: 불용어를 제거할지 여부 선택. 기본값 False\n",
        "  # stop_words: 불용어 사전은 사용자가 직접 입력, 기본값 빈 리스트\n",
        "\n",
        "  # 1. 한글 및 공백 제외한 문자 모두 제거\n",
        "  review_text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]','',review)\n",
        "\n",
        "  #2. okt 객체를 활용하여 형태소 단어로 나눔\n",
        "  word_review = okt.morphs(review_text,stem=True)\n",
        "\n",
        "  if remove_stopwords:\n",
        "    #3. 불용어 제거(선택)\n",
        "    word_review = [token for token in word_review if not token in stop_words]\n",
        "  return word_review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1sRXamS1qLV",
        "outputId": "c75bb089-1d75-4338-d30a-586206eaa89e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['더빙', '진짜', '짜증나다', '목소리'],\n",
              " ['흠', '포스터', '보고', '초딩', '영화', '줄', '오버', '연기', '조차', '가볍다', '않다'],\n",
              " ['너', '무재', '밓었', '다그', '래서', '보다', '추천', '다'],\n",
              " ['교도소', '이야기', '구먼', '솔직하다', '재미', '없다', '평점', '조정']]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 전체 텍스트 전처리\n",
        "stop_words = ['은','는','이','가','하','아','것','들','의','있','되','수','보','주','등','한']\n",
        "okt = Okt()\n",
        "clean_train_review = []\n",
        "\n",
        "for review in train_data['document']:\n",
        "  # 리뷰가 문자열인 경우만 전처리 진행\n",
        "  if type(review) == str:\n",
        "    clean_train_review.append(preprocessing(review,okt,remove_stopwords=True,stop_words= stop_words))\n",
        "  else:\n",
        "    clean_train_review.append([]) #str이 아닌 행은 빈칸으로 놔두기\n",
        "\n",
        "clean_train_review[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygmNtmOv1qLW"
      },
      "outputs": [],
      "source": [
        "#테스트 리뷰도 동일하게 전처리\n",
        "test_data = pd.read_csv(DATA_PATH + 'ratings_test.txt', header = 0, delimiter='\\t', quoting=3)\n",
        "\n",
        "clean_test_review = []\n",
        "for review in test_data['document']:\n",
        "  if type(review) == str:\n",
        "    clean_test_review.append(preprocessing(review, okt, remove_stopwords=True, stop_words=stop_words))\n",
        "  else:\n",
        "    clean_test_review.append([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mmPTLfP1qLW"
      },
      "outputs": [],
      "source": [
        "# 인덱스 벡터 변환 후 일정 길이 넘어가거나 모자라는 리뷰 패딩처리\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_train_review)\n",
        "train_sequences = tokenizer.texts_to_sequences(clean_train_review)\n",
        "test_sequences = tokenizer.texts_to_sequences(clean_test_review)\n",
        "\n",
        "word_vocab = tokenizer.word_index #단어사전형태\n",
        "MAX_SEQUENCE_LENGTH = 8 #문장 최대 길이\n",
        "\n",
        "#학습 데이터\n",
        "train_inputs = pad_sequences(train_sequences,maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
        "\n",
        "#학습 데이터 라벨 벡터화\n",
        "train_labels = np.array(train_data['label'])\n",
        "\n",
        "#평가 데이터\n",
        "test_inputs = pad_sequences(test_sequences,maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
        "#평가 데이터 라벨 벡터화\n",
        "test_labels = np.array(test_data['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTRs9tED1qLW"
      },
      "outputs": [],
      "source": [
        "DEFAULT_PATH  = './content/sample_data/' # 경로지정\n",
        "DATA_PATH = 'CLEAN_DATA/' #.npy파일 저장 경로지정\n",
        "TRAIN_INPUT_DATA = 'nsmc_train_input.npy'\n",
        "TRAIN_LABEL_DATA = 'nsmc_train_label.npy'\n",
        "TEST_INPUT_DATA = 'nsmc_test_input.npy'\n",
        "TEST_LABEL_DATA = 'nsmc_test_label.npy'\n",
        "DATA_CONFIGS = 'data_configs.json'\n",
        "\n",
        "data_configs={}\n",
        "data_configs['vocab'] = word_vocab\n",
        "data_configs['vocab_size'] = len(word_vocab) + 1\n",
        "\n",
        "#전처리한 데이터들 파일로저장\n",
        "import os\n",
        "\n",
        "if not os.path.exists(DEFAULT_PATH + DATA_PATH):\n",
        "  os.makedirs(DEFAULT_PATH+DATA_PATH)\n",
        "\n",
        "#전처리 학습데이터 넘파이로 저장\n",
        "np.save(open(DEFAULT_PATH+DATA_PATH+TRAIN_INPUT_DATA,'wb'),train_inputs)\n",
        "np.save(open(DEFAULT_PATH+DATA_PATH+TRAIN_LABEL_DATA,'wb'),train_labels)\n",
        "#전처리 테스트데이터 넘파이로 저장\n",
        "np.save(open(DEFAULT_PATH+DATA_PATH+TEST_INPUT_DATA,'wb'),test_inputs)\n",
        "np.save(open(DEFAULT_PATH+DATA_PATH+TEST_LABEL_DATA,'wb'),test_labels)\n",
        "\n",
        "#데이터 사전 json으로 저장\n",
        "json.dump(data_configs,open(DEFAULT_PATH + DATA_PATH + DATA_CONFIGS,'w'),ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kN66WTTt1qLX"
      },
      "outputs": [],
      "source": [
        "# 학습 데이터 불러오기\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "#전처리 데이터 불러오기\n",
        "DATA_PATH = './content/sample_data/CLEAN_DATA/'\n",
        "DATA_OUT = './content/sample_data/DATA_OUT/'\n",
        "INPUT_TRAIN_DATA = 'nsmc_train_input.npy'\n",
        "LABEL_TRAIN_DATA = 'nsmc_train_label.npy'\n",
        "DATA_CONFIGS = 'data_configs.json'\n",
        "\n",
        "train_input = np.load(open(DATA_PATH + INPUT_TRAIN_DATA,'rb'))\n",
        "train_input = pad_sequences(train_input,maxlen=train_input.shape[1])\n",
        "train_label = np.load(open(DATA_PATH + LABEL_TRAIN_DATA,'rb'))\n",
        "prepro_configs = json.load(open(DATA_PATH+DATA_CONFIGS,'r'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA13rpMs1qLX",
        "outputId": "19b9ed53-2b78-4759-c03b-6e667d90f375"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting matplotlib\n",
            "  Obtaining dependency information for matplotlib from https://files.pythonhosted.org/packages/59/e2/6b155713e8da8274367b41315b66260f9d3c52d9f4b26336ddc3986ba612/matplotlib-3.8.1-cp311-cp311-win_amd64.whl.metadata\n",
            "  Downloading matplotlib-3.8.1-cp311-cp311-win_amd64.whl.metadata (5.9 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/ca/2a/d197a412ec474391ee878b1218cf2fe9c6e963903755887fc5654c06636a/contourpy-1.2.0-cp311-cp311-win_amd64.whl.metadata\n",
            "  Downloading contourpy-1.2.0-cp311-cp311-win_amd64.whl.metadata (5.8 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Obtaining dependency information for cycler>=0.10 from https://files.pythonhosted.org/packages/e7/05/c19819d5e3d95294a6f5947fb9b9629efb316b96de511b418c53d245aae6/cycler-0.12.1-py3-none-any.whl.metadata\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Obtaining dependency information for fonttools>=4.22.0 from https://files.pythonhosted.org/packages/a0/90/e58e06130ffaf7859959a128fe11ad6533d3e5f4ecf65ecba7464981afdd/fonttools-4.44.0-cp311-cp311-win_amd64.whl.metadata\n",
            "  Downloading fonttools-4.44.0-cp311-cp311-win_amd64.whl.metadata (156 kB)\n",
            "     ---------------------------------------- 0.0/156.8 kB ? eta -:--:--\n",
            "     ------- ----------------------------- 30.7/156.8 kB 660.6 kB/s eta 0:00:01\n",
            "     -------------------------------------- 156.8/156.8 kB 1.9 MB/s eta 0:00:00\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Obtaining dependency information for kiwisolver>=1.3.1 from https://files.pythonhosted.org/packages/1e/37/d3c2d4ba2719059a0f12730947bbe1ad5ee8bff89e8c35319dcb2c9ddb4c/kiwisolver-1.4.5-cp311-cp311-win_amd64.whl.metadata\n",
            "  Downloading kiwisolver-1.4.5-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\user\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from matplotlib) (1.26.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\user\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from matplotlib) (10.1.0)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
            "  Obtaining dependency information for pyparsing>=2.3.1 from https://files.pythonhosted.org/packages/39/92/8486ede85fcc088f1b3dba4ce92dd29d126fd96b0008ea213167940a2475/pyparsing-3.1.1-py3-none-any.whl.metadata\n",
            "  Downloading pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\user\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Downloading matplotlib-3.8.1-cp311-cp311-win_amd64.whl (7.6 MB)\n",
            "   ---------------------------------------- 0.0/7.6 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.4/7.6 MB 8.5 MB/s eta 0:00:01\n",
            "   --- ------------------------------------ 0.7/7.6 MB 8.4 MB/s eta 0:00:01\n",
            "   ----- ---------------------------------- 1.1/7.6 MB 7.8 MB/s eta 0:00:01\n",
            "   -------- ------------------------------- 1.5/7.6 MB 8.2 MB/s eta 0:00:01\n",
            "   ---------- ----------------------------- 2.0/7.6 MB 8.6 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 2.6/7.6 MB 9.4 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 3.0/7.6 MB 9.5 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 3.6/7.6 MB 9.5 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 4.2/7.6 MB 10.0 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 5.0/7.6 MB 10.6 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 5.7/7.6 MB 10.9 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 6.2/7.6 MB 11.0 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 7.0/7.6 MB 11.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  7.6/7.6 MB 11.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 7.6/7.6 MB 11.4 MB/s eta 0:00:00\n",
            "Downloading contourpy-1.2.0-cp311-cp311-win_amd64.whl (187 kB)\n",
            "   ---------------------------------------- 0.0/187.6 kB ? eta -:--:--\n",
            "   ---------------------------------------- 187.6/187.6 kB ? eta 0:00:00\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.44.0-cp311-cp311-win_amd64.whl (2.1 MB)\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   --------------- ------------------------ 0.8/2.1 MB 17.6 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 2.0/2.1 MB 21.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.1/2.1 MB 19.6 MB/s eta 0:00:00\n",
            "Downloading kiwisolver-1.4.5-cp311-cp311-win_amd64.whl (56 kB)\n",
            "   ---------------------------------------- 0.0/56.1 kB ? eta -:--:--\n",
            "   ---------------------------------------- 56.1/56.1 kB 2.9 MB/s eta 0:00:00\n",
            "Downloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
            "   ---------------------------------------- 0.0/103.1 kB ? eta -:--:--\n",
            "   ---------------------------------------- 103.1/103.1 kB ? eta 0:00:00\n",
            "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
            "Successfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.44.0 kiwisolver-1.4.5 matplotlib-3.8.1 pyparsing-3.1.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "! pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaDyytrX1qLX"
      },
      "outputs": [],
      "source": [
        "model_name= 'cnn_classifier_kr'\n",
        "BATCH_SIZE = 512\n",
        "NUM_EPOCHS = 10\n",
        "VALID_SPLIT = 0.1\n",
        "MAX_LEN = train_input.shape[1]\n",
        "\n",
        "kargs={'model_name': model_name, 'vocab_size':prepro_configs['vocab_size'],'embbeding_size':128, 'num_filters':100,'dropout_rate':0.5, 'hidden_dimension':250,'output_dimension':1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8q1CQxa1qLY"
      },
      "outputs": [],
      "source": [
        "class CNNClassifier(tf.keras.Model):\n",
        "\n",
        "\tdef __init__(self, **kargs):\n",
        "\t\tsuper(CNNClassifier, self).__init__(name=kargs['model_name'])\n",
        "\t\tself.embedding = layers.Embedding(input_dim=kargs['vocab_size'], output_dim=kargs['embbeding_size'])\n",
        "\t\tself.conv_list = [layers.Conv1D(filters=kargs['num_filters'], kernel_size=kernel_size, padding='valid',activation = tf.keras.activations.relu,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_constraint = tf.keras.constraints.MaxNorm(max_value=3)) for kernel_size in [3,4,5]]\n",
        "\t\tself.pooling = layers.GlobalMaxPooling1D()\n",
        "\t\tself.dropout = layers.Dropout(kargs['dropout_rate'])\n",
        "\t\tself.fc1 = layers.Dense(units=kargs['hidden_dimension'],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tactivation = tf.keras.activations.relu,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))\n",
        "\t\tself.fc2 = layers.Dense(units=kargs['output_dimension'],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tactivation=tf.keras.activations.sigmoid,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_constraint= tf.keras.constraints.MaxNorm(max_value=3.))\n",
        "\n",
        "\n",
        "\tdef call(self,x):\n",
        "\t\tx = self.embedding(x)\n",
        "\t\tx = self.dropout(x)\n",
        "\t\tx = tf.concat([self.pooling(conv(x)) for conv in self.conv_list], axis = 1)\n",
        "\t\tx = self.fc1(x)\n",
        "\t\tx = self.fc2(x)\n",
        "\t\treturn x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huecdP2Q1qLY",
        "outputId": "5129085e-22e1-491c-8fbf-feb2ae05d0cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./content/sample_data/DATA_OUT/cnn_classifier_kr -- Folder already exists \n",
            "\n",
            "Epoch 1/10\n",
            "264/264 [==============================] - ETA: 0s - loss: 0.4594 - accuracy: 0.7749\n",
            "Epoch 1: val_accuracy improved from -inf to 0.82173, saving model to ./content/sample_data/DATA_OUT/cnn_classifier_kr\\weights.h5\n",
            "264/264 [==============================] - 16s 55ms/step - loss: 0.4594 - accuracy: 0.7749 - val_loss: 0.3901 - val_accuracy: 0.8217\n",
            "Epoch 2/10\n",
            "263/264 [============================>.] - ETA: 0s - loss: 0.3529 - accuracy: 0.8446\n",
            "Epoch 2: val_accuracy improved from 0.82173 to 0.82960, saving model to ./content/sample_data/DATA_OUT/cnn_classifier_kr\\weights.h5\n",
            "264/264 [==============================] - 14s 52ms/step - loss: 0.3529 - accuracy: 0.8446 - val_loss: 0.3811 - val_accuracy: 0.8296\n",
            "Epoch 3/10\n",
            "263/264 [============================>.] - ETA: 0s - loss: 0.2993 - accuracy: 0.8729\n",
            "Epoch 3: val_accuracy did not improve from 0.82960\n",
            "264/264 [==============================] - 14s 53ms/step - loss: 0.2993 - accuracy: 0.8729 - val_loss: 0.3879 - val_accuracy: 0.8295\n",
            "Epoch 4/10\n",
            "263/264 [============================>.] - ETA: 0s - loss: 0.2560 - accuracy: 0.8932\n",
            "Epoch 4: val_accuracy did not improve from 0.82960\n",
            "264/264 [==============================] - 14s 52ms/step - loss: 0.2561 - accuracy: 0.8931 - val_loss: 0.4178 - val_accuracy: 0.8269\n"
          ]
        },
        {
          "ename": "PermissionDeniedError",
          "evalue": "{{function_node __wrapped__MergeV2Checkpoints_device_/job:localhost/replica:0/task:0/device:CPU:0}} Failed to create a directory: /; Permission denied [Op:MergeV2Checkpoints]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\User\\Desktop\\20912 변정수\\SW프로젝트2023\\code\\model.ipynb 셀 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/20912%20%EB%B3%80%EC%A0%95%EC%88%98/SW%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B82023/code/model.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(train_input, train_label, batch_size\u001b[39m=\u001b[39mBATCH_SIZE, epochs \u001b[39m=\u001b[39m NUM_EPOCHS,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/20912%20%EB%B3%80%EC%A0%95%EC%88%98/SW%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B82023/code/model.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m                     validation_split\u001b[39m=\u001b[39mVALID_SPLIT, callbacks\u001b[39m=\u001b[39m[earlystop_callback, cp_callback])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/20912%20%EB%B3%80%EC%A0%95%EC%88%98/SW%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B82023/code/model.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# 모델 저장하기\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/20912%20%EB%B3%80%EC%A0%95%EC%88%98/SW%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B82023/code/model.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m save_model(model,\u001b[39m'\u001b[39;49m\u001b[39m/model\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\User\\PycharmProjects\\pythonProject\\venv\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:167\u001b[0m, in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, save_format, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m     saving_lib\u001b[39m.\u001b[39msave_model(model, filepath)\n\u001b[0;32m    165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    166\u001b[0m     \u001b[39m# Legacy case\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m     \u001b[39mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[39m.\u001b[39;49msave_model(\n\u001b[0;32m    168\u001b[0m         model,\n\u001b[0;32m    169\u001b[0m         filepath,\n\u001b[0;32m    170\u001b[0m         overwrite\u001b[39m=\u001b[39;49moverwrite,\n\u001b[0;32m    171\u001b[0m         save_format\u001b[39m=\u001b[39;49msave_format,\n\u001b[0;32m    172\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    173\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\User\\PycharmProjects\\pythonProject\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\Users\\User\\PycharmProjects\\pythonProject\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mPermissionDeniedError\u001b[0m: {{function_node __wrapped__MergeV2Checkpoints_device_/job:localhost/replica:0/task:0/device:CPU:0}} Failed to create a directory: /; Permission denied [Op:MergeV2Checkpoints]"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import save_model\n",
        "\n",
        "model = CNNClassifier(**kargs)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss = tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics = [tf.keras.metrics.BinaryAccuracy(name='accuracy')])\n",
        "\n",
        "#검증 정확도를 통한 EarlyStopping 기능 및 모델 저장 방식 지정\n",
        "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=2)\n",
        "checkpoint_path = DATA_OUT + model_name +'\\weights.h5'\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "if os.path.exists(checkpoint_dir):\n",
        "  print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
        "else:\n",
        "  os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "  print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
        "\n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, monitor = 'val_accuracy', verbose=1, save_best_only = True,\n",
        "    save_weights_only=True\n",
        ")\n",
        "\n",
        "history = model.fit(train_input, train_label, batch_size=BATCH_SIZE, epochs = NUM_EPOCHS,\n",
        "                    validation_split=VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])\n",
        "# 모델 저장하기\n",
        "save_model(model,'/model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0-8dkW71qLY",
        "outputId": "f37714bf-2a1b-4e45-f365-45df702c7e64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3844 - accuracy: 0.8269\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.38441741466522217, 0.8269400000572205]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "INPUT_TEST_DATA = 'nsmc_test_input.npy'\n",
        "LABEL_TEST_DATA = 'nsmc_test_label.npy'\n",
        "SAVE_FILE_NM = 'weights.h5'\n",
        "\n",
        "test_input = np.load(open(DATA_PATH+INPUT_TEST_DATA,'rb'))\n",
        "test_input = pad_sequences(test_input,maxlen=test_input.shape[1])\n",
        "test_label_data = np.load(open(DATA_PATH + LABEL_TEST_DATA, 'rb'))\n",
        "\n",
        "model.load_weights('content\\sample_data\\DATA_OUT\\cnn_classifier_kr\\weights.h5')\n",
        "model.evaluate(test_input, test_label_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5aChg6h1qLY",
        "outputId": "f83c5153-2d1f-4ca9-c3e1-42cddca7258a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['정수', '아이기차나 httpsdiscordggPHQYwEgn 으 디코에다가 아니면 드라이브에 가람 으 일단 크롤링 나 유튜브 보고 해보겠음 어떻게하는지 어떻게할건지 엉 코드 써놓게 으 하는사람 올라온나 윤서는 안하고 가람이도 집가고 너도 집가서 하자 집에서 디코로 하자는 의견이 6시 1020 난 이쯤 밥 다막을듯 석식 일단 그 디코에 코드좀 조 이따 할거 파이참이나 비주얼스튜디오에서 핍 안되는거 해결법 찾음 정윤서 너도 나중에 가능 물냉먹어야지 어맞다 기 코드 시작하겟슴 동영상 아 메세지 보내는걸 주석처리안해서 자꾸보내저다 사진 계속 짼다 얘 쥐피티가 낫다 하씨 사진 오호 1장만 하는 이유가 있었군 아니지 첫번째거 해야겟네 저거는 몸풀기 저거 조금만 봐라 애들 시키는거도 그거만시키고 사진 사진 이건 뭘까 엄 사진 사진 사진 httpsdrivegooglecomdrivefolders1cRNQr2tBOhrxv5OFr6vwrXImod59vcDJ ', '나윤', '7시 이후에 만나영 그렇네 없다 음 코드는 좀 부탁혀 자료조사 열심히 해보겠음 헤이 다들 과제하고 있어 가람이는 뭐하니 디코 오자 그럼 나도 할 수 있겠다 걍 시간되는 애들 틈틈이 와서 하자 나도 밥 먹고 갈께 나 는 아 무 것도 먹지 않을려고 해 서경아 나랑 나눠먹으실 일단 메뉴보고 난 닭꼬치 하나 먹어야쥐 간단하다 아휴 신나 9일은 아무도 안 되는뎁 나빼고 가람이 있구나 나 궁금한거 있음 13일에 야자가 없는걸로 알고 있거든 그러면 그 때 특강 남아서 하는건가 야자없으니까 학교마치고 바로 시작하는건가 좀 있다가 보고서 쓰면 봐주십쇼 근데 일단 서론이랑 이론적 배경만 쓰면 되겠지 힘내용 늦었는 시간에 죄송하지만 보고서 서론 쓴거 봐주숍 사진 ', '정윤서', '근데 디스코드 방있음 근데 디코에 1명 안들어왔음 아 이모티콘 다 해볼라고 했는데 좀 아닌데 그래도 해보긴 해봐야 하지 않겠나 ', '정서경', '걍 알아서 하고 넣어두는거 맞나 그거 가 초대는 했는데 디코에서 쥬앙 코드도 짜가야되나 자료찾는거아님 나 슬슬 하려고 강가람 디코 오실 어칼겨 어칼까 몇시에 갈까 난 집가는중 나는 좀 하다가 뭐할지 확더우고하면 주말에 이어서할듯 오늘 하긴할거임 나 밖이야 집가서 둘게 줄 머먹을거냐 아 맞다 오므라이스 아님 닭꼬치 먹을듯 상관업스 오므라이스 말고 딴거먹으면 따로먹자 오 이모티콘 변정수 인공지능수학 안듣냐 머고 이모티콘 밥먹고 하겠지 아잇 이거 다 깔아야되네 이모티콘 멀라용 오류가 끝나지 않아용 너 드라이브 뒤져바 링크 겁나 김 아마 ', '강가람', '코드 인터넷에서 보는중 다 이슴 언제 오냐 다들 이모티콘 난 7시쯤 학원가서 그전에 할듯 집가서 보냄 나 면쪽 먹을까 고민중 난 머먹지 그냥 얼큰어묵우동 먹을깨 오 나도 물냉 먹을까 굳 배고파 우리 ai공유 폴더 있는곳 링크 아는사람 찾음 ', '']\n",
            "5/5 [==============================] - 0s 14ms/step\n",
            "정수 :58.43% 확률로 긍정 리뷰입니다.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11484\\2797500077.py:96: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  predictions = float(predictions[0])  # 첫 번째 요소를 스칼라로 변환\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 0s 4ms/step\n",
            "나윤 :52.02% 확률로 긍정 리뷰입니다.\n",
            "\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "정윤서 :82.95% 확률로 긍정 리뷰입니다.\n",
            "\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "정서경 :36.80% 확률로 긍정 리뷰입니다.\n",
            "\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "강가람 :18.44% 확률로 긍정 리뷰입니다.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from tkinter import *\n",
        "import pyautogui\n",
        "import pyperclip\n",
        "import time\n",
        "import os\n",
        "import pretext\n",
        "def InputText(Text):\n",
        "    pyperclip.copy(Text)\n",
        "    pyautogui.hotkey(\"ctrl\", \"v\")\n",
        "\n",
        "def EnterKey():\n",
        "    pyautogui.press(\"Enter\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ChatName = input('채팅방 이름 입력 : ')\n",
        "\n",
        "kakao_path = 'C:/Users/User/KakaoTalk/KakaoTalk.exe'\n",
        "\n",
        "os.system(kakao_path)\n",
        "time.sleep(3)\n",
        "\n",
        "if os.path.exists(f\"ChatTextDataStorage/{ChatName}ChatTextData.txt\"):\n",
        "  pass\n",
        "else:\n",
        "  ChatImg = pyautogui.locateOnScreen('./ButtonImages/Chat.png')\n",
        "\n",
        "  pyautogui.click(ChatImg)\n",
        "  time.sleep(0.1)\n",
        "\n",
        "  pyautogui.hotkey(\"ctrl\", \"f\")\n",
        "  InputText(ChatName)\n",
        "\n",
        "  # 방법1 좌표클릭\n",
        "  \"\"\"pyautogui.moveRel(150,30)\n",
        "  time.sleep(0.5)\n",
        "  pyautogui.doubleClick()\"\"\"\n",
        "\n",
        "  #방법2 걍 엔터버튼\n",
        "  time.sleep(0.5)\n",
        "  EnterKey()\n",
        "\n",
        "\n",
        "\n",
        "  pyautogui.hotkey(\"ctrl\", \"s\")\n",
        "  time.sleep(1)\n",
        "\n",
        "  InputText(f\"{ChatName}ChatTextData.txt\")\n",
        "  time.sleep(5)\n",
        "\n",
        "  EnterKey()\n",
        "  EnterKey()\n",
        "\n",
        "\n",
        "  TextData = f\"ChatTextDataStorage/{ChatName}ChatTextData.txt\"\n",
        "\n",
        "\n",
        "  # Example usage:\n",
        "  output_file = \"ChatTextDataStorage/sen.txt\"\n",
        "  pretext.process_text_data(TextData, output_file)\n",
        "\n",
        "ReadTextData =[]\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "okt = Okt()\n",
        "tokenizer  = Tokenizer()\n",
        "\n",
        "prepro_configs = json.load(open('content\\sample_data\\CLEAN_DATA\\data_configs.json','r'))\n",
        "prepro_configs['vocab'] = word_vocab\n",
        "\n",
        "tokenizer.fit_on_texts(word_vocab)\n",
        "\n",
        "MAX_LENGTH = 8 #문장최대길이\n",
        "\n",
        "with open(output_file, \"r\", encoding='UTF8') as f:\n",
        "        ReadTextData = f.read().split(\"\\n\")\n",
        "\n",
        "for i, sentence in enumerate(ReadTextData):\n",
        "  if i % 2 == 0:\n",
        "     user = ReadTextData[i]\n",
        "     pass\n",
        "  else :\n",
        "    sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣\\\\s ]','', sentence)\n",
        "    stopwords = ['은','는','이','가','하','아','것','들','의','있','되','수','보','주','등','한'] # 불용어 추가할 것이 있으면 이곳에 추가\n",
        "    sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    sentence = [word for word in sentence if not word in stopwords] # 불용어 제거\n",
        "    vector  = tokenizer.texts_to_sequences(sentence)\n",
        "    pad_new = pad_sequences(vector, maxlen = MAX_LENGTH) # 패딩\n",
        "\n",
        "    model.load_weights('content\\sample_data\\DATA_OUT\\cnn_classifier_kr\\weights.h5') #모델 불러오기\n",
        "    predictions = model.predict(pad_new)\n",
        "    predictions = float(predictions[0])  # 첫 번째 요소를 스칼라로 변환\n",
        "\n",
        "    print(f\"{user} :\" \"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(predictions * 100))\n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sWD8P2h1qLY",
        "outputId": "dd0d0e40-1455-4051-827e-5b8ef60e0226"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 0s 3ms/step\n",
            "정수 :58.43% 확률로 긍정 리뷰입니다.\n",
            "\n",
            "['아이', '기차', '나', 'ss', '으', '디코', '에다가', '아니다', '드라이브', '에', '가람', '으', '일단', '크롤', '링', '나', '유튜브', '보고', '해보다', '어떻다', '하다', '어떻다', '하다', '건지다', '엉', '코드', '써다', '으', '하다', '사람', '올라오다', '윤서', '안', '하고', '가람이', '도', '지다', '너', '도', '지다', '하자', '집', '에서', '디', '코로', '하자', '의견', '시', '난', '이쯤', '밥', '다', '막다', '석식', '일단', '그', '디코', '에', '코드', '좀', '조', '이따', '하다', '파이', '차다', '비주얼스튜디오', '에서', '피다', '안되다', '해결', '법', '찾다', '정윤서', '너', '도', '나중', '에', '가능', '물냉', '먹다', '어맞다', '기', '코드', '시작', '겟슴', '동영상', '메세지', '보내다', '석', '처리', '안해', '서', '자꾸', '보내다', '저', '다', '사진', '계속', '째다', '얘', '쥐피티', '낫다', '하씨', '사진', '오호', '장만', '하다', '이유', '있다', '아니다', '첫', '번째', '거', '하다', '저', '거', '몸', '풀기', '저', '거', '조금', '만', '보다', '애', '시키다', '그거', '만', '시키다', '사진', '사진', '이건', '뭘', '끄다', '엄', '사진', '사진', '사진', 'ss']\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "나윤 :52.02% 확률로 긍정 리뷰입니다.\n",
            "\n",
            "['시', '이후', '에', '만', '나영', '그렇다', '없다', '음', '코드', '좀', '부탁', '혀', '자료', '조사', '열심히', '해보다', '헤이', '다', '들다', '과제', '하고', '있다', '가람이', '뭐', '하다', '디코', '오자', '그렇다', '나다', '하다', '있다', '걍', '시간', '되다', '애', '틈틈이', '오다', '하자', '나다', '밥', '먹다', '갈다', '나', '늘다', '무', '도', '먹다', '않다', '해', '서경', '나', '랑', '나누다', '먹다', '일단', '메뉴', '보고', '난', '닭꼬치', '하나', '먹다', '쥐', '간단하다', '휴', '신나다', '일', '아무', '도', '안', '되는뎁', '나', '빼다', '가람이', '있다', '나', '궁금하다', '있다', '일', '에', '야자', '없다', '는걸', '로', '알', '고', '있다', '그러면', '그', '때', '특강', '남아', '서', '하다', '건가', '야자', '없다', '학교', '마치', '고', '바로', '시작', '하다', '건가', '좀', '있다', '보고서', '쓰다', '보다', '쇼', '근데', '일단', '서론', '이랑', '이론', '적', '배경', '만', '쓰다', '되다', '힘내다', '늦다', '시간', '에', '죄송하다', '보고서', '서론', '쓸다', '봐주숍', '사진']\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "정윤서 :82.95% 확률로 긍정 리뷰입니다.\n",
            "\n",
            "['근데', '디스', '코드', '방', '있다', '근데', '디코', '에', '명', '안', '들어오다', '이모티콘', '다', '해보다', '하다', '좀', '아니다', '그래도', '해보다', '해보다', '하다', '않다']\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "정서경 :36.80% 확률로 긍정 리뷰입니다.\n",
            "\n",
            "['걍', '알다', '하다', '넣다', '두다', '맞다', '그거', '가다', '초대', '하다', '디코', '에서', '쥬앙', '코드', '도', '짜다', '나', '자료', '찾다', '아니다', '나', '슬슬', '하다', '강', '가람', '디코', '오다', '어칼겨', '어칼까', '몇', '시', '에', '갈다', '난', '지다', '중', '나', '좀', '하다', '뭐', '하다', '확', '덥다', '주말', '에', '이어서', '하다', '오늘', '하다', '하다', '임', '나', '밖', '이야', '지다', '둘', '게', '줄', '머', '먹다', '맞다', '오므라이스', '아니다', '닭꼬치', '먹다', '상관', '업스', '오므라이스', '말고', '따다', '먹다', '따로', '먹다', '오', '이모티콘', '변정수', '인공', '지능', '수학', '안', '듣다', '머', '고', '이모티콘', '밥', '먹다', '하다', '잇다', '거', '다', '깔다', '네', '이모티콘', '멀다', '라용', '오류', '끝나다', '않다', '너', '드라이브', '뒤지다', '바', '링크', '겁나다', '김', '아마']\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "강가람 :18.44% 확률로 긍정 리뷰입니다.\n",
            "\n",
            "['코드', '인터넷', '에서', '보다', '중', '다', '이슴', '언제', '오냐', '다', '들다', '이모티콘', '난', '시', '쯤', '학원', '가다', '그', '전', '에', '하다', '지다', '보내다', '나', '면쪽', '먹다', '고민', '중', '난', '머', '먹다', '그냥', '얼', '크다', '어묵', '우동', '먹다', '깨', '오', '나다', '물냉', '먹다', '굳다', '배고프다', '우리', '공유', '폴더', '있다', '곳', '링크', '알다', '사람', '찾다']\n"
          ]
        }
      ],
      "source": [
        "for i, sentence in enumerate(ReadTextData):\n",
        "  if i % 2 == 0:\n",
        "     user = ReadTextData[i]\n",
        "     pass\n",
        "  else :\n",
        "    sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣\\\\s ]','', sentence)\n",
        "    stopwords = ['은','는','이','가','하','아','것','들','의','있','되','수','보','주','등','한'] # 불용어 추가할 것이 있으면 이곳에 추가\n",
        "    sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    sentence = [word for word in sentence if not word in stopwords] # 불용어 제거\n",
        "    vector  = tokenizer.texts_to_sequences(sentence)\n",
        "    pad_new = pad_sequences(vector, maxlen = MAX_LENGTH) # 패딩\n",
        "\n",
        "    model.load_weights('content\\sample_data\\DATA_OUT\\cnn_classifier_kr\\weights.h5') #모델 불러오기\n",
        "    predictions = model.predict(pad_new)\n",
        "    predictions = predictions[0].item()  # 배열의 값을 스칼라로 변환\n",
        "  # 첫 번째 요소를 스칼라로 변환\n",
        "\n",
        "\n",
        "    print(f\"{user} :\" \"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(predictions * 100))\n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZOhB6wG1qLY"
      },
      "outputs": [],
      "source": [
        "for k, v in dict.items():\n",
        "    p = 0\n",
        "    for s in v:\n",
        "        p += pre(v)\n",
        "        if p > 0.5:\n",
        "            print(f\"{k} : {p * 100} 확률로 긍정\")\n",
        "        else :\n",
        "            print(f\"{k} : {(1 - p) * 100} 확률로 부정\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XInvf7pq1qLY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}