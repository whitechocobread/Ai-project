{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whitechocobread/Ai-project/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sj_7aW-z1qLS",
        "outputId": "9be4bd91-7c6e-4832-ed46-57661bee9fac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyautogui\n",
            "  Downloading PyAutoGUI-0.9.54.tar.gz (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymsgbox (from pyautogui)\n",
            "  Downloading PyMsgBox-1.0.9.tar.gz (18 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytweening>=1.0.4 (from pyautogui)\n",
            "  Downloading pytweening-1.0.7.tar.gz (168 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyscreeze>=0.1.21 (from pyautogui)\n",
            "  Downloading PyScreeze-0.1.30.tar.gz (27 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygetwindow>=0.0.5 (from pyautogui)\n",
            "  Downloading PyGetWindow-0.0.9.tar.gz (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mouseinfo (from pyautogui)\n",
            "  Downloading MouseInfo-0.1.3.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python3-Xlib (from pyautogui)\n",
            "  Downloading python3-xlib-0.15.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyrect (from pygetwindow>=0.0.5->pyautogui)\n",
            "  Downloading PyRect-0.2.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=9.2.0 in /usr/local/lib/python3.10/dist-packages (from pyscreeze>=0.1.21->pyautogui) (9.4.0)\n",
            "Requirement already satisfied: pyperclip in /usr/local/lib/python3.10/dist-packages (from mouseinfo->pyautogui) (1.8.2)\n",
            "Building wheels for collected packages: pyautogui, pygetwindow, pyscreeze, pytweening, mouseinfo, pymsgbox, python3-Xlib, pyrect\n",
            "  Building wheel for pyautogui (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyautogui: filename=PyAutoGUI-0.9.54-py3-none-any.whl size=37576 sha256=7cf0bd1d4081c9d361c394d3742e542546f823bef61ca815db60c0dce4136d6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/a7/1c/5a51aaff3bbe110be4ddf766d429cc9d2fae7a72fc1b843e56\n",
            "  Building wheel for pygetwindow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pygetwindow: filename=PyGetWindow-0.0.9-py3-none-any.whl size=11062 sha256=e52e6ecd607cac739d769adabcd2e2fe8cc420836b75ebed3093b5fd444130ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/f6/64/c5d427819f80553df2398bfecc351e94e00371c1dcb6edb24e\n",
            "  Building wheel for pyscreeze (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyscreeze: filename=PyScreeze-0.1.30-py3-none-any.whl size=14382 sha256=26fe6af71781757de3461786bc2b37cd174b69569c63dc2c43045e88f3a3a520\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/46/42/d3613adf9880c8b4a780c07a8cf85df12ca04a9a0ca77c6c02\n",
            "  Building wheel for pytweening (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytweening: filename=pytweening-1.0.7-py3-none-any.whl size=6197 sha256=9291b37a816fdc778d56c69ef223e5b6c551271e14001f5d9bd123f58d39178c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/a3/d7/36c45539416215425b3247e37b691a98a03b1db7b13b7f9632\n",
            "  Building wheel for mouseinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mouseinfo: filename=MouseInfo-0.1.3-py3-none-any.whl size=10890 sha256=21eb7a42d238f2db23f19aead6de49be8e422905118b7632f1f857515f4d929f\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/d5/27/2f1be84b3e6ccee99c82f50e3fe7fe6360dd30417109b49a72\n",
            "  Building wheel for pymsgbox (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymsgbox: filename=PyMsgBox-1.0.9-py3-none-any.whl size=7406 sha256=8e31495769fe6c240252815a9bc384a07611e456c90b5cbc9dd1a3f2196b43dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/6a/ba/be2d7d78166ec8018c21d07241dffa54446c09652a267759ae\n",
            "  Building wheel for python3-Xlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python3-Xlib: filename=python3_xlib-0.15-py3-none-any.whl size=109500 sha256=e9ed4ace77f8027462dfa4f46e19059c55e1089a6f77652e3245ffe7740dd3d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/8e/c5/8a93a3415a4a2065f31750a3244db61482b9e508f04ee56e82\n",
            "  Building wheel for pyrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrect: filename=PyRect-0.2.0-py2.py3-none-any.whl size=11179 sha256=1ff364bdee70be43173e01b18ef451aee643c60dafccb95c26333751727bd80a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/4c/bd/42e4e23641afcd185d4e932784da37e6e04505da0cf3f7b832\n",
            "Successfully built pyautogui pygetwindow pyscreeze pytweening mouseinfo pymsgbox python3-Xlib pyrect\n",
            "Installing collected packages: pytweening, python3-Xlib, pyrect, pymsgbox, pyscreeze, pygetwindow, mouseinfo, pyautogui\n",
            "Successfully installed mouseinfo-0.1.3 pyautogui-0.9.54 pygetwindow-0.0.9 pymsgbox-1.0.9 pyrect-0.2.0 pyscreeze-0.1.30 python3-Xlib-0.15 pytweening-1.0.7\n"
          ]
        }
      ],
      "source": [
        "! pip install pyautogui"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WbIcktQ6lU99",
        "outputId": "9c9031ee-4b3c-4675-db10-a5d946ce0364",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy\n"
      ],
      "metadata": {
        "id": "psdMO0F3lxew",
        "outputId": "f07226a3-c579-4c84-cbe6-3f8b68de3b74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hvrozsi01qLU",
        "outputId": "314e4b5f-e681-402d-f024-08ac6ad2f0cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   labels                            Description \n",
            "0      -2         삼성전자 폴더블폰 시장 점유율이 86%에서 72%로 하락\n",
            "1       1  글로벌 폴더블폰 시장이 전년 대비 16%, 전분기 대비 215% 증가\n",
            "2      -1         중국 업체들의 공격적인 제품 출시로 삼성전자 점유율 하락\n",
            "3       2      삼성 갤럭시Z 플립5와 폴드5, 시장에서 가장 잘 팔리는 모델\n",
            "4       1  4분기 삼성전자 폴더블폰 점유율, 작년 83%에서 42%로 하락 예상\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "file_path = '/content/drive/MyDrive/삼성전자.xlsx'\n",
        "df = pd.read_excel(file_path, header = 0)\n",
        "\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_df = df['Description ']\n",
        "y_df = df['labels']\n",
        "print('본문의 개수: {}'.format(len(X_df)))\n",
        "print('레이블의 개수: {}'.format(len(y_df)))\n",
        ""
      ],
      "metadata": {
        "id": "FnqV5AViq-Ok",
        "outputId": "f0472282-c708-4b2e-fc91-9bc510436058",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "본문의 개수: 110\n",
            "레이블의 개수: 110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=0, stratify=y_df)\n",
        "# train_data와 test_data 대신 X_train, X_test 사용\n",
        "print(f\"학습 데이터 크기: {len(X_train)}\\n테스트 데이터 크기: {len(X_test)}\")"
      ],
      "metadata": {
        "id": "JT5Z_weXq6nX",
        "outputId": "7d6ae9f6-ba21-4748-976a-30d62a1cc0ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 데이터 크기: 88\n",
            "테스트 데이터 크기: 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "bPfcfWTi1qLV",
        "outputId": "2f9dd357-7ad1-4069-cf63-b4308078f518",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87          [갤럭시, 방수, 방진, 등급, 먼지, 와, 습기, 걱정, 없다]\n",
            "11          [직원, 에게, 년, 목표, 설정, 과, 이정표, 세우다, 권장]\n",
            "67                   [고성능, 컴퓨팅, 시장, 투자, 필요성, 증가]\n",
            "21    [삼성, 후광, 효과, 로, 용인, 화성, 수원, 지역, 집값, 에, 영향]\n",
            "Name: Description , dtype: object\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# 전처리 함수\n",
        "def preprocessing(review, okt, remove_stopwords=False, stop_words=[]):\n",
        "    # 한국어가 아닌 문자 제거\n",
        "    review_text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]', '', review)\n",
        "\n",
        "    # 형태소로 토큰화\n",
        "    word_review = okt.morphs(review_text, stem=True)\n",
        "\n",
        "    # 필요한 경우 불용어 제거\n",
        "    if remove_stopwords:\n",
        "        word_review = [token for token in word_review if token not in stop_words]\n",
        "\n",
        "    return word_review\n",
        "\n",
        "# Okt 객체 생성 및 불용어 설정\n",
        "okt = Okt()\n",
        "stop_words = ['은', '는', '이', '가', '하', '아', '것', '들', '의', '있', '되', '수', '보', '주', '등', '한']\n",
        "\n",
        "# 학습 데이터 전처리\n",
        "clean_train_review = X_train.apply(lambda x: preprocessing(x, okt, remove_stopwords=True, stop_words=stop_words) if isinstance(x, str) else [])\n",
        "\n",
        "# 결과 확인\n",
        "print(clean_train_review[:4])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ygmNtmOv1qLW",
        "outputId": "50407310-1469-4924-ad65-49f4ee7a779d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1          [글로벌, 폴, 더블, 폰, 시장, 전, 년, 대비, 전, 분기, 대비, 증가]\n",
            "16              [직원, 일상, 업무, 에, 너무, 집중, 하다, 말다, 하다, 조언]\n",
            "19    [하이닉스, 와의, 시장, 경쟁, 에서, 삼, 성, 전자, 수익, 성, 개선, 요원...\n",
            "46                  [삼성, 전자, 임원, 장기, 성과, 보수, 제도, 운영, 중]\n",
            "Name: Description , dtype: object\n"
          ]
        }
      ],
      "source": [
        "# 테스트 데이터에 전처리 함수 적용\n",
        "clean_test_review = X_test.apply(lambda x: preprocessing(x, okt, remove_stopwords=True, stop_words=stop_words) if isinstance(x, str) else [])\n",
        "\n",
        "# 결과 확인\n",
        "print(clean_test_review[:4])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# 인덱스 벡터 변환 및 패딩 처리\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_train_review)\n",
        "train_sequences = tokenizer.texts_to_sequences(clean_train_review)\n",
        "test_sequences = tokenizer.texts_to_sequences(clean_test_review)\n",
        "\n",
        "word_vocab = tokenizer.word_index # 단어 사전 형태\n",
        "MAX_SEQUENCE_LENGTH = 8 # 문장 최대 길이\n",
        "\n",
        "# 학습 데이터\n",
        "train_inputs = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "\n",
        "# 학습 데이터 라벨 벡터화\n",
        "train_labels = np.array(y_train)\n",
        "\n",
        "# 평가 데이터\n",
        "test_inputs = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "\n",
        "# 평가 데이터 라벨 벡터화\n",
        "test_labels = np.array(y_test)\n"
      ],
      "metadata": {
        "id": "84nf1IReudlp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "kN66WTTt1qLX"
      },
      "outputs": [],
      "source": [
        "# 학습 데이터 불러오기\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "#전처리 데이터 불러오기\n",
        "DATA_PATH = './content/sample_data/CLEAN_DATA/'\n",
        "DATA_OUT = './content/sample_data/DATA_OUT/'\n",
        "INPUT_TRAIN_DATA = 'nsmc_train_input.npy'\n",
        "LABEL_TRAIN_DATA = 'nsmc_train_label.npy'\n",
        "DATA_CONFIGS = 'data_configs.json'\n",
        "\n",
        "train_input = np.load(open(DATA_PATH + INPUT_TRAIN_DATA,'rb'))\n",
        "train_input = pad_sequences(train_input,maxlen=train_input.shape[1])\n",
        "train_label = np.load(open(DATA_PATH + LABEL_TRAIN_DATA,'rb'))\n",
        "prepro_configs = json.load(open(DATA_PATH+DATA_CONFIGS,'r'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "LA13rpMs1qLX",
        "outputId": "2c17f9a8-8d65-4ea6-d2c7-ace9802e8cf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "AaDyytrX1qLX"
      },
      "outputs": [],
      "source": [
        "model_name= 'cnn_classifier_kr'\n",
        "BATCH_SIZE = 512\n",
        "NUM_EPOCHS = 10\n",
        "VALID_SPLIT = 0.1\n",
        "MAX_LEN = train_input.shape[1]\n",
        "\n",
        "kargs={'model_name': model_name, 'vocab_size':prepro_configs['vocab_size'],'embbeding_size':128, 'num_filters':100,'dropout_rate':0.5, 'hidden_dimension':250,'output_dimension':1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "C8q1CQxa1qLY"
      },
      "outputs": [],
      "source": [
        "class CNNClassifier(tf.keras.Model):\n",
        "\n",
        "\tdef __init__(self, **kargs):\n",
        "\t\tsuper(CNNClassifier, self).__init__(name=kargs['model_name'])\n",
        "\t\tself.embedding = layers.Embedding(input_dim=kargs['vocab_size'], output_dim=kargs['embbeding_size'])\n",
        "\t\tself.conv_list = [layers.Conv1D(filters=kargs['num_filters'], kernel_size=kernel_size, padding='valid',activation = tf.keras.activations.relu,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_constraint = tf.keras.constraints.MaxNorm(max_value=3)) for kernel_size in [3,4,5]]\n",
        "\t\tself.pooling = layers.GlobalMaxPooling1D()\n",
        "\t\tself.dropout = layers.Dropout(kargs['dropout_rate'])\n",
        "\t\tself.fc1 = layers.Dense(units=kargs['hidden_dimension'],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tactivation = tf.keras.activations.relu,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))\n",
        "\t\tself.fc2 = layers.Dense(units=kargs['output_dimension'],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tactivation=tf.keras.activations.sigmoid,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_constraint= tf.keras.constraints.MaxNorm(max_value=3.))\n",
        "\n",
        "\n",
        "\tdef call(self,x):\n",
        "\t\tx = self.embedding(x)\n",
        "\t\tx = self.dropout(x)\n",
        "\t\tx = tf.concat([self.pooling(conv(x)) for conv in self.conv_list], axis = 1)\n",
        "\t\tx = self.fc1(x)\n",
        "\t\tx = self.fc2(x)\n",
        "\t\treturn x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "huecdP2Q1qLY",
        "outputId": "86c331ed-bc13-404e-8edd-d2d5b0ed5e00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./content/sample_data/DATA_OUT -- Folder create complete \n",
            "\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6927 - accuracy: 0.1266\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00000, saving model to ./content/sample_data/DATA_OUT/cnn_classifier_kr\\weights.h5\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6927 - accuracy: 0.1266 - val_loss: 0.6906 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6771 - accuracy: 0.1646\n",
            "Epoch 2: val_accuracy improved from 0.00000 to 0.11111, saving model to ./content/sample_data/DATA_OUT/cnn_classifier_kr\\weights.h5\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.6771 - accuracy: 0.1646 - val_loss: 0.6970 - val_accuracy: 0.1111\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6620 - accuracy: 0.2785\n",
            "Epoch 3: val_accuracy improved from 0.11111 to 0.22222, saving model to ./content/sample_data/DATA_OUT/cnn_classifier_kr\\weights.h5\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.6620 - accuracy: 0.2785 - val_loss: 0.7045 - val_accuracy: 0.2222\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6365 - accuracy: 0.3671\n",
            "Epoch 4: val_accuracy improved from 0.22222 to 0.33333, saving model to ./content/sample_data/DATA_OUT/cnn_classifier_kr\\weights.h5\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.6365 - accuracy: 0.3671 - val_loss: 0.7109 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6251 - accuracy: 0.3544\n",
            "Epoch 5: val_accuracy did not improve from 0.33333\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.6251 - accuracy: 0.3544 - val_loss: 0.7158 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5948 - accuracy: 0.4051\n",
            "Epoch 6: val_accuracy did not improve from 0.33333\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.5948 - accuracy: 0.4051 - val_loss: 0.7196 - val_accuracy: 0.3333\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import save_model\n",
        "\n",
        "model = CNNClassifier(**kargs)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss = tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics = [tf.keras.metrics.BinaryAccuracy(name='accuracy')])\n",
        "\n",
        "#검증 정확도를 통한 EarlyStopping 기능 및 모델 저장 방식 지정\n",
        "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=2)\n",
        "checkpoint_path = DATA_OUT + model_name +'\\weights.h5'\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "if os.path.exists(checkpoint_dir):\n",
        "  print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
        "else:\n",
        "  os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "  print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
        "\n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, monitor = 'val_accuracy', verbose=1, save_best_only = True,\n",
        "    save_weights_only=True\n",
        ")\n",
        "\n",
        "history = model.fit(train_input, train_label, batch_size=BATCH_SIZE, epochs = NUM_EPOCHS,\n",
        "                    validation_split=VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])\n",
        "# 모델 저장하기\n",
        "save_model(model,'/model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "1ZOhB6wG1qLY",
        "outputId": "8c7e3fae-aad2-4231-9926-26456b20bc1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-d32d15fb2760>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# 예측 함수를 model.predict로 변경하고, 적절한 데이터 형식으로 변환 필요\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unbound method dict.items() needs an argument"
          ]
        }
      ],
      "source": [
        "# 예시: 전처리된 테스트 데이터가 있다고 가정\n",
        "test_data_preprocessed = ... # 전처리된 테스트 데이터\n",
        "\n",
        "# 모델 예측\n",
        "predictions = model.predict(test_data_preprocessed)\n",
        "\n",
        "# 결과 해석 및 출력\n",
        "for i, prediction in enumerate(predictions):\n",
        "    if prediction > 0.5:\n",
        "        print(f\"샘플 {i}: 긍정 ({prediction[0] * 100:.2f}%)\")\n",
        "    else:\n",
        "        print(f\"샘플 {i}: 부정 ({(1 - prediction[0]) * 100:.2f}%)\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}